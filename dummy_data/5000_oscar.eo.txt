Help on TextGenerationPipeline in module transformers.pipelines.text_generation object:

class TextGenerationPipeline(transformers.pipelines.base.Pipeline)
 |  TextGenerationPipeline(*args, **kwargs)
 |  
 |  Language generation pipeline using any `ModelWithLMHead`. This pipeline predicts the words that will follow a
 |  specified text prompt. When the underlying model is a conversational model, it can also accept one or more chats,
 |  in which case the pipeline will operate in chat mode and will continue the chat(s) by adding its response(s).
 |  Each chat takes the form of a list of dicts, where each dict contains "role" and "content" keys.
 |  
 |  Examples:
 |  
 |  ```python
 |  >>> from transformers import pipeline
 |  
 |  >>> generator = pipeline(model="openai-community/gpt2")
 |  >>> generator("I can't believe you did such a ", do_sample=False)
 |  [{'generated_text': "I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I"}]
 |  
 |  >>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.
 |  >>> outputs = generator("My tart needs some", num_return_sequences=4, return_full_text=False)
 |  ```
 |  
 |  ```python
 |  >>> from transformers import pipeline
 |  
 |  >>> generator = pipeline(model="HuggingFaceH4/zephyr-7b-beta")
 |  >>> # Zephyr-beta is a conversational model, so let's pass it a chat instead of a single string
 |  >>> generator([{"role": "user", "content": "What is the capital of France? Answer in one word."}], do_sample=False, max_new_tokens=2)
 |  [{'generated_text': [{'role': 'user', 'content': 'What is the capital of France? Answer in one word.'}, {'role': 'assistant', 'content': 'Paris'}]}]
 |  ```
 |  
 |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial). You can pass text
 |  generation parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about
 |  text generation parameters in [Text generation strategies](../generation_strategies) and [Text
 |  generation](text_generation).
 |  
 |  This language generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:
 |  `"text-generation"`.
 |  
 |  The models that this pipeline can use are models that have been trained with an autoregressive language modeling
 |  objective. See the list of available [text completion models](https://huggingface.co/models?filter=text-generation)
 |  and the list of [conversational models](https://huggingface.co/models?other=conversational)
 |  on [huggingface.co/models].
 |  
 |  Arguments:
 |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):
 |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from
 |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.
 |      tokenizer ([`PreTrainedTokenizer`]):
 |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from
 |          [`PreTrainedTokenizer`].
 |      modelcard (`str` or [`ModelCard`], *optional*):
 |          Model card attributed to the model for this pipeline.
 |      framework (`str`, *optional*):
 |          The framework to use, either `"pt"` for PyTorch or `"tf"` for TensorFlow. The specified framework must be
 |          installed.
 |  
 |          If no framework is specified, will default to the one currently installed. If no framework is specified and
 |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is
 |          provided.
 |      task (`str`, defaults to `""`):
 |          A task-identifier for the pipeline.
 |      num_workers (`int`, *optional*, defaults to 8):
 |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of
 |          workers to be used.
 |      batch_size (`int`, *optional*, defaults to 1):
 |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
 |          the batch to use, for inference this is not always beneficial, please read [Batching with
 |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .
 |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):
 |          Reference to the object in charge of parsing supplied pipeline parameters.
 |      device (`int`, *optional*, defaults to -1):
 |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on
 |          the associated CUDA device id. You can pass native `torch.device` or a `str` too
 |      torch_dtype (`str` or `torch.dtype`, *optional*):
 |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model
 |          (`torch.float16`, `torch.bfloat16`, ... or `"auto"`)
 |      binary_output (`bool`, *optional*, defaults to `False`):
 |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as
 |          the raw output data e.g. text.
 |  
 |  Method resolution order:
 |      TextGenerationPipeline
 |      transformers.pipelines.base.Pipeline
 |      transformers.pipelines.base._ScikitCompat
 |      abc.ABC
 |      transformers.utils.hub.PushToHubMixin
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __call__(self, text_inputs, **kwargs)
 |      Complete the prompt(s) given as inputs.
 |      
 |      Args:
 |          text_inputs (`str`, `List[str]`, List[Dict[str, str]], or `List[List[Dict[str, str]]]`):
 |              One or several prompts (or one list of prompts) to complete. If strings or a list of string are
 |              passed, this pipeline will continue each prompt. Alternatively, a "chat", in the form of a list
 |              of dicts with "role" and "content" keys, can be passed, or a list of such chats. When chats are passed,
 |              the model's chat template will be used to format them before passing them to the model.
 |          return_tensors (`bool`, *optional*, defaults to `False`):
 |              Whether or not to return the tensors of predictions (as token indices) in the outputs. If set to
 |              `True`, the decoded text is not returned.
 |          return_text (`bool`, *optional*, defaults to `True`):
 |              Whether or not to return the decoded texts in the outputs.
 |          return_full_text (`bool`, *optional*, defaults to `True`):
 |              If set to `False` only added text is returned, otherwise the full text is returned. Only meaningful if
 |              *return_text* is set to True.
 |          clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):
 |              Whether or not to clean up the potential extra spaces in the text output.
 |          prefix (`str`, *optional*):
 |              Prefix added to prompt.
 |          handle_long_generation (`str`, *optional*):
 |              By default, this pipelines does not handle long generation (ones that exceed in one form or the other
 |              the model maximum length). There is no perfect way to adress this (more info
 |              :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common
 |              strategies to work around that problem depending on your use case.
 |      
 |              - `None` : default strategy where nothing in particular happens
 |              - `"hole"`: Truncates left of input, and leaves a gap wide enough to let generation happen (might
 |                truncate a lot of the prompt and not suitable when generation exceed the model capacity)
 |          generate_kwargs (`dict`, *optional*):
 |              Additional keyword arguments to pass along to the generate method of the model (see the generate method
 |              corresponding to your framework [here](./main_classes/text_generation)).
 |      
 |      Return:
 |          A list or a list of lists of `dict`: Returns one of the following dictionaries (cannot return a combination
 |          of both `generated_text` and `generated_token_ids`):
 |      
 |          - **generated_text** (`str`, present when `return_text=True`) -- The generated text.
 |          - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token
 |            ids of the generated text.
 |  
 |  __init__(self, *args, **kwargs)
 |      Initialize self.  See help(type(self)) for accurate signature.
 |  
 |  postprocess(self, model_outputs, return_type=<ReturnType.FULL_TEXT: 2>, clean_up_tokenization_spaces=True)
 |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into
 |      something more friendly. Generally it will output a list or a dict or results (containing just strings and
 |      numbers).
 |  
 |  preprocess(self, prompt_text, prefix='', handle_long_generation=None, add_special_tokens=None, truncation=None, padding=None, max_length=None, **generate_kwargs)
 |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for
 |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes defined here:
 |  
 |  XL_PREFIX = '\n    In 1991, the remains of Russian Tsar Nichol...  beg...
 |  
 |  __abstractmethods__ = frozenset()
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from transformers.pipelines.base.Pipeline:
 |  
 |  check_model_type(self, supported_models: Union[List[str], dict])
 |      Check if the model class is in supported by the pipeline.
 |      
 |      Args:
 |          supported_models (`List[str]` or `dict`):
 |              The list of models supported by the pipeline, or a dictionary with model class values.
 |  
 |  device_placement(self)
 |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.
 |      
 |      Returns:
 |          Context manager
 |      
 |      Examples:
 |      
 |      ```python
 |      # Explicitly ask for tensor allocation on CUDA device :0
 |      pipe = pipeline(..., device=0)
 |      with pipe.device_placement():
 |          # Every framework specific tensor allocation will be done on the request device
 |          output = pipe(...)
 |      ```
 |  
 |  ensure_tensor_on_device(self, **inputs)
 |      Ensure PyTorch tensors are on the specified device.
 |      
 |      Args:
 |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):
 |              The tensors to place on `self.device`.
 |          Recursive on lists **only**.
 |      
 |      Return:
 |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.
 |  
 |  forward(self, model_inputs, **forward_params)
 |  
 |  get_inference_context(self)
 |  
 |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)
 |  
 |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)
 |  
 |  predict(self, X)
 |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().
 |  
 |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, tags: Optional[List[str]] = None, **deprecated_kwargs) -> str
 |      Upload the pipeline file to the ðŸ¤— Model Hub.
 |      
 |      Parameters:
 |          repo_id (`str`):
 |              The name of the repository you want to push your pipe to. It should contain your organization name
 |              when pushing to a given organization.
 |          use_temp_dir (`bool`, *optional*):
 |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.
 |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.
 |          commit_message (`str`, *optional*):
 |              Message to commit while pushing. Will default to `"Upload pipe"`.
 |          private (`bool`, *optional*):
 |              Whether or not the repository created should be private.
 |          token (`bool` or `str`, *optional*):
 |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
 |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`
 |              is not specified.
 |          max_shard_size (`int` or `str`, *optional*, defaults to `"5GB"`):
 |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard
 |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed
 |              by a unit (like `"5MB"`). We default it to `"5GB"` so that users can easily load models on free-tier
 |              Google Colab instances without any CPU OOM issues.
 |          create_pr (`bool`, *optional*, defaults to `False`):
 |              Whether or not to create a PR with the uploaded files or directly commit.
 |          safe_serialization (`bool`, *optional*, defaults to `True`):
 |              Whether or not to convert the model weights in safetensors format for safer serialization.
 |          revision (`str`, *optional*):
 |              Branch to push the uploaded files to.
 |          commit_description (`str`, *optional*):
 |              The description of the commit that will be created
 |          tags (`List[str]`, *optional*):
 |              List of tags to push on the Hub.
 |      
 |      Examples:
 |      
 |      ```python
 |      from transformers import pipeline
 |      
 |      pipe = pipeline("google-bert/bert-base-cased")
 |      
 |      # Push the pipe to your namespace with the name "my-finetuned-bert".
 |      pipe.push_to_hub("my-finetuned-bert")
 |      
 |      # Push the pipe to an organization with the name "my-finetuned-bert".
 |      pipe.push_to_hub("huggingface/my-finetuned-bert")
 |      ```
 |  
 |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)
 |  
 |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
 |  
 |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, **kwargs)
 |      Save the pipeline's model and tokenizer.
 |      
 |      Args:
 |          save_directory (`str` or `os.PathLike`):
 |              A path to the directory where to saved. It will be created if it doesn't exist.
 |          safe_serialization (`str`):
 |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.
 |          kwargs (`Dict[str, Any]`, *optional*):
 |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.
 |  
 |  transform(self, X)
 |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().
 |  
 |  ----------------------------------------------------------------------
 |  Readonly properties inherited from transformers.pipelines.base.Pipeline:
 |  
 |  torch_dtype
 |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:
 |  
 |  default_input_names = None
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
